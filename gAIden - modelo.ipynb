{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gAIden - Jogando Ninja Gaiden via Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Criando nosso ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos todas as bibliotecas que utilizaremos:\n",
    "import tensorflow as tf  \n",
    "import numpy as np\n",
    "import retro\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray # Help us to gray our frames\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "from collections import deque# Ordered collection with ends\n",
    "import random\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criamos nosso ambiente:\n",
    "env = retro.make(game='NinjaGaiden-Nes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estudamos os tamanhos de nossos frames e ações:\n",
    "print('O tamanho de cada frame é:', env.observation_space)\n",
    "print('Nosso espaço de ações tem tamanho :', env.action_space.n)\n",
    "\n",
    "#Fazemos um one-hot-encoding para nossas ações:\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Processamento e Stack de frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos a função de pré-processamento de frames:\n",
    "\n",
    "def preprocess(frame):\n",
    "    preto_e_branco=rgb2gray(frame)    #converte o frame em preto e branco\n",
    "    corte=preto_e_branco[45:-12,6:-12] #corta pedaços da tela que não estamos usando\n",
    "    norm=corte/255                    #normaliza o frame\n",
    "    frame_processado=transform.resize(norm,[110,84]) \n",
    "    \n",
    "    return frame_processado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de processarmos nossa imagem, ela era:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "print(imshow(rgb2gray(env.reset())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do pré-processamento, ela se torna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imshow(preprocess(env.reset())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos fazer um stack de  4 frames pré-processados. Inicializamos este stack todo nulo\n",
    "n_stacks=4 \n",
    "stacked=deque([np.zeros((110,84), dtype=np.int) for i in range(n_stacks)], maxlen=4)\n",
    "\n",
    "def stack_de_frames(stacked, state, indicador_de_episodio): #indicador_de_episodio é uma variável binária que indica se é ou não um novo episódio\n",
    "#Primeiro vamos pré-processar:    \n",
    "    processar=preprocess(state)\n",
    "    \n",
    "#Se for um novo episódio, limparemos o stack e faremos nosso novo stack:    \n",
    "    if indicador_de_episodio:\n",
    "#Limpa o stack:\n",
    "        stacked=deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "#Coloca o mesmo frame 4 vezes no stack em questão:\n",
    "        stacked.append(state)\n",
    "        stacked.append(state)\n",
    "        stacked.append(state)\n",
    "        stacked.append(state)\n",
    "#Faz o stack via numpy:        \n",
    "        stacked_state = np.stack(stacked, axis=2)\n",
    "\n",
    "#Se não for um novo episódio, eliminamos o frame mais antigo do stack e colocamos nosso frame atual\n",
    "    else:\n",
    "        stacked.append(state)\n",
    "        stacked_state = np.stack(stacked, axis=2)\n",
    "    \n",
    "    return stacked_state, stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Seleção de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparâmetros que poderemos ou não variar para avaliar a performance do nosso modelo\n",
    "state_size = [110, 84, 4]      \n",
    "action_size = env.action_space.n\n",
    "learning_rate =  0.00025      \n",
    "total_episodes = 50            \n",
    "max_steps = 50000              \n",
    "batch_size = 64                \n",
    "explore_start = 1.0            \n",
    "explore_stop = 0.01             \n",
    "decay_rate = 0.00001           \n",
    "gamma = 0.9                    \n",
    "pretrain_length = batch_size   \n",
    "memory_size = 1000000          \n",
    "stack_size = 4                 \n",
    "tau=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Desenvolvendo nossa Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em termos de arquitetura, nossa rede neural será composta da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.optimizers import Adam \n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, env):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.episodes = total_episodes\n",
    "        self.env._max_episode_steps = max_steps\n",
    "        self.epsilon = explore_start\n",
    "        self.epsilon_decay = decay_rate\n",
    "        self.epsilon_min = explore_stop\n",
    "        self.gamma = gamma\n",
    "        self.alpha = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self.build_model()\n",
    "        self.tau=tau\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.Input(shape=(110,84,1)))\n",
    "        model.add(Conv2D(filters = 32, kernel_size = [8,8], strides = [4,4], \n",
    "                         padding='valid',kernel_initializer=\"glorot_uniform\"))\n",
    "        model.add(Conv2D(filters = 64, kernel_size = [4,4], strides = [2,2], \n",
    "                         padding='valid',kernel_initializer=\"glorot_uniform\"))\n",
    "        model.add(Conv2D(filters = 64, kernel_size = [3,3], strides = [2,2], \n",
    "                         padding='valid',kernel_initializer=\"glorot_uniform\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def act(self, state):\n",
    "        if (np.random.random() <= self.epsilon):\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state))\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "    def rreplay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "                \n",
    "            self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def target_train(self):\n",
    "        #Construir função de treino\n",
    "        \n",
    "    \n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinando e vendo nosso agente evoluir.\n",
    "dqn_agent = DQNAgent(env=env)\n",
    "steps = []\n",
    "\n",
    "done=False\n",
    "state=env.reset()\n",
    "i=0\n",
    "\n",
    "for i in range(0,total_episodes):\n",
    "    print('geração {}'.format(i))\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    while not done:\n",
    "        state=preprocess(state)\n",
    "        action = dqn_agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        dqn_agent.remember(state, action, reward, next_state, done)\n",
    "        state=next_state\n",
    "        env.render()\n",
    "        dqn_agent.target_train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
